
<div align="center">
<img src="./assets/bleeding.png" style="width: 30%;height: 30%">
</div>

# Model-Hemorrhage
Model Hemorrhage and the Robustness Limits of Large Language  Models: A Perspective

## ğŸ“œ Paper
Our full paper is available on arXiv: [**Model Hemorrhage and the Robustness Limits of Large Language Models**](https://doi.org/10.48550/arXiv.2503.23924) [![Paper](https://img.shields.io/badge/Paper-%F0%9F%8E%93-lightblue?style=flat-square)](https://doi.org/10.48550/arXiv.2503.23924)

## ğŸ“¬ Contact
If you find any errors or have suggestions, feel free to reach out: **maziyang@whu.edu.cn**

---

## ğŸŒ About
In our work, we introduce the concept of Model Hemorrhage, a comprehensive framework that investigates how optimization techniquesâ€”such as pruning, quantization, and decoding adaptationsâ€”can lead to unexpected degradation in performance and stability. Through empirical analysis and theoretical insights, we reveal five key dimensions of fragility in LLMs: architectural redundancy, model compression, training-inference, extension mechanisms, and data-related vulnerabilities. To ground this framework, we present empirical case studies that illuminate key trade-offsâ€”including cardinal sparsity thresholds for pruning, the lossless quantization thresholds, full-size progressive quantization, and horizontal comparisons across different compression strategies and decoding methods.

